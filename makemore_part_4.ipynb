{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBN5v1JpIQ4qiLx0zGZSO2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woodRock/makemore/blob/main/makemore_part_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngh5Ic9ms5OJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read in all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "words[:8]"
      ],
      "metadata": {
        "id": "OrEx02ajs8Ti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd953231-a8e0-41da-cea1-8914a55e5614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "id": "Taetkoohs-DR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4cb2ced-4ff5-4e14-a246-6f6d68a93296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32033"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "862lBoLQtBOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e784b909-5e34-4b94-f0f3-c5525b2e098b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
      ],
      "metadata": {
        "id": "h8QareZztDKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ac1487a-b294-4a0f-da8f-7b45108870b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ],
      "metadata": {
        "id": "quIylWTuvGzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "# Note: I am initializating many of these parameters in non-standard ways\n",
        "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
        "# implementation of the backward pass.\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW-QGFlzvGPb",
        "outputId": "06ae0edd-abf1-4e33-b736-79705a9cd2f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "n = batch_size # a shorter variable also, for convenience\n",
        "# construct a minibatch\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
      ],
      "metadata": {
        "id": "Mz6GVXkgvIR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
        "\n",
        "emb = C[Xb] # embed the characters into vectors\n",
        "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "# Linear layer 1\n",
        "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "# BatchNorm layer\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "bndiff = hprebn - bnmeani\n",
        "bndiff2 = bndiff**2\n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "bnraw = bndiff * bnvar_inv\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2 # output layer\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5glpZmcvvMaS",
        "outputId": "a8049544-96d7-4f0c-c663-6a2657f1f0ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.3479, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demb.shape, Xb.shape, C.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2JzZto--hm5",
        "outputId": "7231e4f9-51de-460a-f3e6-f8df95bd3497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 3, 10]), torch.Size([32, 3]), torch.Size([27, 10]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1: backprop through the whole thing manually,\n",
        "# backpropagating through exactly all of the variables\n",
        "# as they are defined in the forward pass above, one by one\n",
        "\n",
        "dlogprobs = torch.zeros_like(logprobs)\n",
        "dlogprobs[range(n), Yb] = -1.0/n\n",
        "dprobs = (1.0 / probs) * dlogprobs\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
        "dcounts = counts_sum_inv * dprobs\n",
        "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
        "dcounts += torch.ones_like(counts) * dcounts_sum\n",
        "dnorm_logits = counts * dcounts\n",
        "dlogits = dnorm_logits.clone()\n",
        "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
        "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
        "dh = dlogits @ W2.T\n",
        "dW2 = h.T @ dlogits\n",
        "db2 = dlogits.sum(0)\n",
        "dhpreact = (1.0 - h**2) * dh\n",
        "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "dbnraw = bngain * dhpreact\n",
        "dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "dbndiff = bnvar_inv * dbnraw\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "dbndiff += (2*bndiff) * dbndiff2\n",
        "dhprebn = dbndiff.clone()\n",
        "dbnmeani = (-dbndiff).sum(0)\n",
        "dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "dembcat = dhprebn @ W1.T\n",
        "dW1 = embcat.T @ dhprebn\n",
        "db1 = dhprebn.sum(0)\n",
        "demb = dembcat.view(emb.shape)\n",
        "# dC = torch.zeros_like(C)\n",
        "# for k in range(Xb.shape[0]):\n",
        "#   for j in range(Xb.shape[1]):\n",
        "#     ix = Xb[k,j]\n",
        "#     dC[ix] += demb[k,j]\n",
        "dC = torch.zeros_like(C)  # shape: [27, 10]\n",
        "dC.index_add_(0, Xb.reshape(-1), demb.reshape(-1, C.shape[1]))\n",
        "\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "cmp('probs', dprobs, probs)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "cmp('counts', dcounts, counts)\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "cmp('logits', dlogits, logits)\n",
        "cmp('h', dh, h)\n",
        "cmp('W2', dW2, W2)\n",
        "cmp('b2', db2, b2)\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "cmp('embcat', dembcat, embcat)\n",
        "cmp('W1', dW1, W1)\n",
        "cmp('b1', db1, b1)\n",
        "cmp('emb', demb, emb)\n",
        "cmp('C', dC, C)"
      ],
      "metadata": {
        "id": "KW-pcNWKU7_R",
        "outputId": "0568bfba-9e28-4392-9da9-795e657c8264",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "bnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bnvar_inv       | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnvar           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bndiff2         | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-11\n",
            "bndiff          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bnmeani         | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "hprebn          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "embcat          | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
            "W1              | exact: False | approximate: True  | maxdiff: 1.30385160446167e-08\n",
            "b1              | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\n",
            "emb             | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
            "C               | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2: backprop through cross_entropy but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the loss,\n",
        "# take the derivative, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# logit_maxes = logits.max(1, keepdim=True).values\n",
        "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "# counts = norm_logits.exp()\n",
        "# counts_sum = counts.sum(1, keepdims=True)\n",
        "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "# probs = counts * counts_sum_inv\n",
        "# logprobs = probs.log()\n",
        "# loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# now:\n",
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYK55l5gvP0d",
        "outputId": "f4f0444c-98e6-48a1-f54e-2dd673bce9b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.347879409790039 diff: -2.384185791015625e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Backward pass\n",
        "\n",
        "dlogits = F.softmax(logits, 1)\n",
        "dlogits[range(n), Yb] -= 1\n",
        "dlogits /= n\n",
        "\n",
        "cmp('logits', dlogits, logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTNqqX9jAZvp",
        "outputId": "7e28e428-3f60-411e-9354-7bc2e294a77a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits          | exact: False | approximate: True  | maxdiff: 6.984919309616089e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlogits[0] * n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jixPcG9GA00N",
        "outputId": "39f52f37-3121-4c4a-ea15-967e06852446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0687,  0.0814,  0.0177,  0.0502,  0.0206,  0.0877,  0.0220,  0.0346,\n",
              "        -0.9843,  0.0317,  0.0422,  0.0345,  0.0405,  0.0276,  0.0376,  0.0147,\n",
              "         0.0086,  0.0192,  0.0157,  0.0526,  0.0507,  0.0236,  0.0242,  0.0691,\n",
              "         0.0609,  0.0255,  0.0223], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(dlogits.detach(), cmap=\"gray\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "id": "iaXClpWdA5EX",
        "outputId": "9813dcbd-bb46-41df-bf8b-dc3399ffc1e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7a37d39c3700>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3: backprop through batchnorm but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
        "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "# bndiff = hprebn - bnmeani\n",
        "# bndiff2 = bndiff**2\n",
        "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "# bnraw = bndiff * bnvar_inv\n",
        "# hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# now:\n",
        "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXirDqUACUOa",
        "outputId": "8c403689-9ae4-4612-f233-c323b8f266cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward pass\n",
        "\n",
        "# before we had:\n",
        "# dbnraw = bngain * dhpreact\n",
        "# dbndiff = bnvar_inv * dbnraw\n",
        "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "# dbndiff += (2*bndiff) * dbndiff2\n",
        "# dhprebn = dbndiff.clone()\n",
        "# dbnmeani = (-dbndiff).sum(0)\n",
        "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "\n",
        "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
        "# (you'll also need to use some of the variables from the forward pass up above)\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "dhprebn = bngain * bnvar_inv / n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "# -----------------\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HszEl-7CvRW-",
        "outputId": "fa2fb1ac-80e3-4db6-e659-7555b16e3cee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hprebn          | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 4: putting it all together!\n",
        "# Train the MLP neural net with your own backward pass\n",
        "\n",
        "# init\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "n = batch_size # convenience\n",
        "lossi = []\n",
        "\n",
        "# use this context manager for efficiency once your backward pass is written (TODO)\n",
        "#with torch.no_grad():\n",
        "\n",
        "# kick off optimization\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xb] # embed the characters into vectors\n",
        "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  # Linear layer\n",
        "  hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "  # BatchNorm layer\n",
        "  # -------------------------------------------------------------\n",
        "  bnmean = hprebn.mean(0, keepdim=True)\n",
        "  bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
        "  bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "  bnraw = (hprebn - bnmean) * bnvar_inv\n",
        "  hpreact = bngain * bnraw + bnbias\n",
        "  # -------------------------------------------------------------\n",
        "  # Non-linearity\n",
        "  h = torch.tanh(hpreact) # hidden layer\n",
        "  logits = h @ W2 + b2 # output layer\n",
        "  loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  # loss.backward() # use this for correctness comparisons, delete it later!\n",
        "\n",
        "  # manual backprop! #swole_doge_meme\n",
        "  # -----------------\n",
        "  # YOUR CODE HERE :)\n",
        "  dlogits = F.softmax(logits, 1)\n",
        "  dlogits[range(n), Yb] -= 1\n",
        "  dlogits /= n\n",
        "  # 2nd layer backprop\n",
        "  dh = dlogits @ W2.T\n",
        "  dW2 = h.T @ dlogits\n",
        "  db2 = dlogits.sum(0)\n",
        "  # tanh\n",
        "  dhpreact = (1.0 - h**2) * dh\n",
        "  # batchnorm backprop\n",
        "  dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "  dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "  dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "  # 1st layer\n",
        "  dembcat = dhprebn @ W1.T\n",
        "  dW1 = embcat.T @ dhprebn\n",
        "  db1 = dhprebn.sum(0)\n",
        "  # embedding\n",
        "  demb = dembcat.view(emb.shape)\n",
        "  dC = torch.zeros_like(C)\n",
        "  for k in range(Xb.shape[0]):\n",
        "    for j in range(Xb.shape[1]):\n",
        "      ix = Xb[k,j]\n",
        "      dC[ix] += demb[k,j]\n",
        "  grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
        "  # -----------------\n",
        "\n",
        "  # update\n",
        "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "  for p, grad in zip(parameters, grads):\n",
        "    # p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
        "    p.data += -lr * grad # new way of swole doge TODO: enable\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n",
        "\n",
        "  # if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
        "    # break"
      ],
      "metadata": {
        "id": "65o0McL5vS2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "718345dc-dfde-4fb2-a889-9a23554e3ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12297\n",
            "      0/ 200000: 3.7706\n",
            "  10000/ 200000: 2.1686\n",
            "  20000/ 200000: 2.3849\n",
            "  30000/ 200000: 2.4231\n",
            "  40000/ 200000: 2.0165\n",
            "  50000/ 200000: 2.3280\n",
            "  60000/ 200000: 2.4042\n",
            "  70000/ 200000: 2.0664\n",
            "  80000/ 200000: 2.3818\n",
            "  90000/ 200000: 2.0338\n",
            " 100000/ 200000: 1.9686\n",
            " 110000/ 200000: 2.3732\n",
            " 120000/ 200000: 2.0756\n",
            " 130000/ 200000: 2.4875\n",
            " 140000/ 200000: 2.3355\n",
            " 150000/ 200000: 2.2169\n",
            " 160000/ 200000: 1.9286\n",
            " 170000/ 200000: 1.7796\n",
            " 180000/ 200000: 1.9238\n",
            " 190000/ 200000: 1.8310\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# useful for checking your gradients\n",
        "# for p,g in zip(parameters, grads):\n",
        "#   cmp(str(tuple(p.shape)), g, p)"
      ],
      "metadata": {
        "id": "M3IJx5tDvUeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calibrate the batch norm at the end of training\n",
        "\n",
        "with torch.no_grad():\n",
        "  # pass the training set through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  # measure the mean/std over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
      ],
      "metadata": {
        "id": "oXtz0rusvVUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate train and val loss\n",
        "\n",
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "metadata": {
        "id": "E9XEcrO7vYiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "\n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # forward pass\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
        "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "      hpreact = embcat @ W1 + b1\n",
        "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "      logits = h @ W2 + b2 # (N, vocab_size)\n",
        "      # sample\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      if ix == 0:\n",
        "        break\n",
        "\n",
        "    print(''.join(itos[i] for i in out))"
      ],
      "metadata": {
        "id": "HA8sQDorvZbW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}